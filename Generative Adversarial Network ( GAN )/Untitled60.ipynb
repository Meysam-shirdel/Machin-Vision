{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyJm2NZsrzVZ"
      },
      "outputs": [],
      "source": [
        "!pip install imageio\n",
        "!pip install git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qxkkmbrrr4gY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from  tensorflow.keras import layers\n",
        "import time\n",
        "from sklearn.utils import shuffle\n",
        "from IPython import display\n",
        "\n",
        "from imutils import build_montages\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KLNjOhtr-PR",
        "outputId": "5e34a14a-de83-4814-a074-7d3264b2808d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train) , (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],1).astype('float32')\n",
        "\n",
        "# Normalization -1 to 1\n",
        "normalized_x_train = ( x_train - (np.max(x_train) / 2) ) / (np.max(x_train) / 2)\n",
        "normalized_x_train = - normalized_x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "khAmLXgJ_KbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420a8a11-f6c4-451f-b985-11d7fba9120d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "((trainX, _), (testX, _)) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "#((trainX, _), (testX, _)) = tf.keras.datasets.mnist.load_data()\n",
        "trainImages = np.concatenate([trainX, testX])\n",
        "trainImages = np.expand_dims(trainImages, axis=-1)\n",
        "trainImages  = (trainImages.astype(\"float\") - 127.5) / 127.5\n",
        "buffer_size = trainImages.shape[0]\n",
        "batch_size = 256\n",
        "noise_dim = 100\n",
        "batch_count = np.floor( trainImages.shape[0] / batch_size).astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0PnThYNsJvv"
      },
      "outputs": [],
      "source": [
        "plt.imshow(normalized_x_train[45], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0T2szRmsKQY"
      },
      "outputs": [],
      "source": [
        "# Shuffle data\n",
        "buffer_size = trainImages.shape[0]\n",
        "batch_size = 256\n",
        "batch_count = np.floor( trainImages.shape[0] / batch_size).astype('int32')\n",
        "shuffled_x_train = tf.data.Dataset.from_tensor_slices(normalized_x_train).shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E2WhOxspsZgQ"
      },
      "outputs": [],
      "source": [
        "class CGAN:\n",
        "  \n",
        "  def __init__(self, buffersize, batchsize):\n",
        "    self.Generator= None\n",
        "    self.Discriminator = None\n",
        "    self.GAN = None\n",
        "    # This method returns a helper function to compute cross entropy loss\n",
        "    '''self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)'''\n",
        "\n",
        "    self.BUFFER_SIZE = buffersize\n",
        "    self.BATCH_SIZE = batchsize\n",
        "    self.batchperepochs = np.floor(self.BUFFER_SIZE / self.BATCH_SIZE).astype('int32')\n",
        "    self.EPOCHS = 50\n",
        "    self.noise_dim = 100\n",
        "    self.num_examples_to_generate = 16\n",
        "    self.lr = 2e-4\n",
        "\n",
        "    # You will reuse this seed overtime (so it's easier)\n",
        "    # to visualize progress in the animated GIF)\n",
        "    self.seed = tf.random.normal([self.num_examples_to_generate, self.noise_dim])\n",
        "\n",
        "    print(\"an empty Generative Adversarial Network model is created...!\")\n",
        "\n",
        "\n",
        "  def make_generator(self,dim=7, depth=64, inputDim=100, outputDim=512, channels=1):\n",
        "    '''\n",
        "    Make an image Generator  \n",
        "    '''\n",
        "    inputShape = (dim, dim, depth)\n",
        "    chanDim = -1\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(input_dim=inputDim, units=outputDim, use_bias= False, input_shape=(100,)))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(7 * 7 * 64))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    \n",
        "    model.add(layers.Reshape(inputShape))\n",
        "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2),padding=\"same\"))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    model.add(layers.BatchNormalization(axis=chanDim))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(channels, (5, 5), strides=(2, 2),padding=\"same\"))\n",
        "    model.add(layers.Activation(\"tanh\"))\n",
        "\n",
        "    print(\"A Generator is added to the GAN model\")\n",
        "    return model\n",
        "\n",
        "    \n",
        "  def make_discriminator(self, alpha=0.2):\n",
        "    '''\n",
        "    Make a Discriminator\n",
        "    '''\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(32, 5 , strides=(2,2), use_bias= False, padding='same', input_shape=[28,28,1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    #model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    #model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512))\n",
        "    model.add(layers.LeakyReLU(alpha=alpha))\n",
        "\n",
        "    model.add(layers.Dense(1))\n",
        "    model.add(layers.Activation(\"sigmoid\"))\n",
        "\n",
        "    print(\"A Discriminator is added to the GAN model\")\n",
        "    return model\n",
        "\n",
        "  def create_GAN(self):\n",
        "    \n",
        "    self.Generator = self.make_generator()\n",
        "    self.Discriminator = self.make_discriminator()\n",
        "    self.Discriminator.compile(optimizer = tf.keras.optimizers.legacy.Adam(0.001,beta_1=0.5, decay=self.lr / self.EPOCHS),loss=\"binary_crossentropy\")\n",
        "    self.Discriminator.trainable = False\n",
        "\n",
        "    ganinput = layers.Input((100,))\n",
        "    ganoutput = self.Discriminator( self.Generator(ganinput))\n",
        "    self.GAN = tf.keras.Model(ganinput, ganoutput)\n",
        "    self.GAN.compile(optimizer= tf.keras.optimizers.legacy.Adam(lr=self.lr, beta_1=0.5, decay=self.lr / self.EPOCHS),loss=\"binary_crossentropy\")\n",
        "    display.clear_output(wait=True)\n",
        "    print(\"A GAN with one generator and one discriminator is created\")\n",
        "    \n",
        "\n",
        "\n",
        "  def train(self, train_images, epochs):\n",
        "    print(\"[INFO] starting training...\")\n",
        "    benchmarkNoise = tf.random.normal((self.BATCH_SIZE, self.noise_dim))\n",
        "    NUM_EPOCHS = self.EPOCHS\n",
        "    BATCH_SIZE = self.BATCH_SIZE\n",
        "    #OUTPUT = args[\"output\"]\n",
        "    for epoch in range(0,epochs):\n",
        "      batchperepochs =  self.batchperepochs #train_images.shape[0] / self.BATCH_SIZE\n",
        "      for j in range(0, batchperepochs):\n",
        "      #j =0\n",
        "      #for batch in train_images:\n",
        "        #j= j+1\n",
        "        p = None\n",
        "        noises = tf.random.normal((self.BATCH_SIZE, self.noise_dim))\n",
        "        generated_images= self.Generator.predict(noises, verbose=0)\n",
        "\n",
        "        imgs = np.concatenate((train_images[j * self.BATCH_SIZE: (j+1) * self.BATCH_SIZE], generated_images ))\n",
        "        #imgs = np.concatenate((batch, generated_images ))\n",
        "        labels = ([1] * self.BATCH_SIZE) + ([0] * self.BATCH_SIZE)\n",
        "        labels = np.reshape(labels, (-1,))\n",
        "        (imgs, labels) = shuffle(imgs, labels)\n",
        "\n",
        "        discloss = self.Discriminator.train_on_batch(imgs,labels)\n",
        "\n",
        "        noises = tf.random.normal((self.BATCH_SIZE, self.noise_dim))\n",
        "        fakelabels = ([1] * self.BATCH_SIZE)\n",
        "        fakelabels = np.reshape(fakelabels, (-1,))\n",
        "\n",
        "        GANloss = self.GAN.train_on_batch(noises, fakelabels)\n",
        "\n",
        "        if j== batchperepochs -1:\n",
        "          p =  [\"epoch_{}_2_output.png\".format(str(epoch + 1).zfill(4))]\n",
        "        elif j == np.int32(batchperepochs/2):\n",
        "          print(j)\n",
        "          p =  [\"epoch_{}_1_output.png\".format(str(epoch + 1).zfill(4))]\n",
        "                \n",
        "        if p is not None:\n",
        "          print(\"[INFO] Step {}_{}: discriminator_loss={:.6f}, \"\n",
        "\t\t\t\t    \"adversarial_loss={:.6f}\".format(epoch + 1, j,\n",
        "\t\t\t\t    \tdiscloss, GANloss))\n",
        "          images = self.Generator.predict(benchmarkNoise)\n",
        "          images = ((images * 127.5) + 127.5).astype(\"uint8\")\n",
        "          images = np.repeat(images, 3, axis=-1)\n",
        "          vis = build_montages(images, (28, 28), (8, 8))[0]\n",
        "          p = os.path.sep.join(p)\n",
        "          cv2.imwrite(p, vis)\n",
        "\n",
        "    plt.imshow(vis)\n",
        "\n",
        "  def predict(self, noises):\n",
        "    predicted_images = self.Generator.predict(noises , verbose=0)\n",
        "    predicted_images = ((predicted_images * 127.5) + 127.5).astype(\"uint8\")\n",
        "    predicted_images = np.repeat(predicted_images, 3, axis=-1)\n",
        "    vis = build_montages(predicted_images, (28, 28), (16, 16))[0]\n",
        "    return vis\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdV__03Pi3WE",
        "outputId": "b507ca2f-e776-4220-f3f2-eaa04d9d2bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A GAN with one generator and one discriminator is created\n"
          ]
        }
      ],
      "source": [
        "cg = CGAN(buffer_size, batch_size)\n",
        "cg.create_GAN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRRTdUd3jPIS"
      },
      "outputs": [],
      "source": [
        "cg.train(trainImages,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5fzFCvMj0JT"
      },
      "outputs": [],
      "source": [
        "test_noises = tf.random.normal((batch_size, noise_dim))\n",
        "image = cg.predict(test_noises)\n",
        "\n",
        "plt.imshow(image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}