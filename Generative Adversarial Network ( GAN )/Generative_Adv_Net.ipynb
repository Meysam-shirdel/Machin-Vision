{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YZ839yf63rSm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from  tensorflow.keras import layers\n",
        "import time\n",
        "from sklearn.utils import shuffle\n",
        "from IPython import display\n",
        "from imutils import build_montages\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset...\n",
        "\n",
        "((trainX, _), (testX, _)) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "#((trainX, _), (testX, _)) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "#combine train & test images to use all the dataset for learning process.\n",
        "trainImages = np.concatenate([trainX, testX])\n",
        "trainImages = np.expand_dims(trainImages, axis=-1)\n",
        "\n",
        "# normalization of images between -1 and 1 \n",
        "trainImages  = (trainImages.astype(\"float\") - 127.5) / 127.5\n",
        "\n",
        "buffer_size = trainImages.shape[0]\n",
        "batch_size = 256 \n",
        "\n",
        "#Noise dimention (100,) is used to make noise dataset (noise images) to give to Generator \n",
        "noise_dim = 100  \n"
      ],
      "metadata": {
        "id": "rb4_J_Nu4C8C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GAN:\n",
        "  \n",
        "#  This is the main class of GAN consists of several methods\n",
        "  def __init__(self, buffersize, batchsize=256, noisedimention=100):\n",
        "    self.GAN = None\n",
        "    self.Generator= None\n",
        "    self.Discriminator = None\n",
        "    \n",
        "    self.BUFFER_SIZE = buffersize\n",
        "    self.BATCH_SIZE = batchsize\n",
        "    self.Batchperepochs = np.floor(self.BUFFER_SIZE / self.BATCH_SIZE).astype('int32')\n",
        "    self.EPOCHS = 50\n",
        "    self.noise_dim = noisedimention\n",
        "    self.lr = 2e-4\n",
        "    self.create_GAN()\n",
        "    print(\"A GAN with one generator and one discriminator is created\")\n",
        "\n",
        "\n",
        "  def make_generator(self,dim=7, depth=64, inputDim=100, outputDim=512, channels=1):\n",
        "    '''\n",
        "    Make an image Generator  \n",
        "    '''\n",
        "    inputShape = (dim, dim, depth)\n",
        "    chanDim = -1\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Dense(input_dim=inputDim, units=outputDim, use_bias= False, input_shape=(100,)))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(7 * 7 * 64))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    \n",
        "    model.add(layers.Reshape(inputShape))\n",
        "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2),padding=\"same\"))\n",
        "    model.add(layers.Activation(\"relu\"))\n",
        "    model.add(layers.BatchNormalization(axis=chanDim))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(channels, (5, 5), strides=(2, 2),padding=\"same\"))\n",
        "    model.add(layers.Activation(\"tanh\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "    \n",
        "  def make_discriminator(self, alpha=0.2):\n",
        "    '''\n",
        "    Make a Discriminator\n",
        "    '''\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(32, 5 , strides=(2,2), use_bias= False, padding='same', input_shape=[28,28,1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(512))\n",
        "    model.add(layers.LeakyReLU(alpha=alpha))\n",
        "\n",
        "    model.add(layers.Dense(1))\n",
        "    model.add(layers.Activation(\"sigmoid\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "  def create_GAN(self):\n",
        "    \n",
        "    self.Generator = self.make_generator()\n",
        "    self.Discriminator = self.make_discriminator()\n",
        "    self.Discriminator.compile(optimizer = tf.keras.optimizers.legacy.Adam(0.001,beta_1=0.5, decay=self.lr / (self.EPOCHS+50)),loss=\"binary_crossentropy\")\n",
        "    self.Discriminator.trainable = False\n",
        "\n",
        "    ganinput = layers.Input((100,))\n",
        "    ganoutput = self.Discriminator( self.Generator(ganinput))\n",
        "    self.GAN = tf.keras.Model(ganinput, ganoutput)\n",
        "    self.GAN.compile(optimizer= tf.keras.optimizers.legacy.Adam(lr=self.lr, beta_1=0.5, decay=self.lr / (self.EPOCHS+50)),loss=\"binary_crossentropy\")\n",
        "    display.clear_output(wait=True) \n",
        "\n",
        "\n",
        "  def train(self, train_images, epochs):\n",
        "    print(\"Start training...\")\n",
        "    benchmarkNoise = tf.random.normal((self.BATCH_SIZE, self.noise_dim))\n",
        "    self.EPOCHS = epochs\n",
        "    \n",
        "    for epoch in range(0,epochs):\n",
        "      for j in range(0, self.Batchperepochs):\n",
        "        p = None\n",
        "        noises = tf.random.normal((self.BATCH_SIZE, self.noise_dim))\n",
        "        generated_images= self.Generator.predict(noises, verbose=0)\n",
        "\n",
        "        imgs = np.concatenate((train_images[j * self.BATCH_SIZE: (j+1) * self.BATCH_SIZE], generated_images ))\n",
        "        #imgs = np.concatenate((batch, generated_images ))\n",
        "        labels = ([1] * self.BATCH_SIZE) + ([0] * self.BATCH_SIZE)\n",
        "        labels = np.reshape(labels, (-1,))\n",
        "        (imgs, labels) = shuffle(imgs, labels)\n",
        "\n",
        "        discloss = self.Discriminator.train_on_batch(imgs,labels)\n",
        "\n",
        "        noises = tf.random.normal((self.BATCH_SIZE, self.noise_dim))\n",
        "        fakelabels = ([1] * self.BATCH_SIZE)\n",
        "        fakelabels = np.reshape(fakelabels, (-1,))\n",
        "\n",
        "        GANloss = self.GAN.train_on_batch(noises, fakelabels)\n",
        "\n",
        "        if j== self.Batchperepochs - 1: \n",
        "          p =  [\"epoch_{}_2_output.png\".format(str(epoch + 1).zfill(4))]\n",
        "        elif j == np.int32(self.Batchperepochs/2):\n",
        "          p =  [\"epoch_{}_1_output.png\".format(str(epoch + 1).zfill(4))]\n",
        "                \n",
        "        if p is not None:\n",
        "          print(\"Step {}_{}: Discriminator_loss={:.6f}, \"\n",
        "\t\t\t\t    \"GAN_loss={:.6f}\".format(epoch + 1, j,\n",
        "\t\t\t\t    \tdiscloss, GANloss))\n",
        "          images = self.Generator.predict(benchmarkNoise)\n",
        "          images = ((images * 127.5) + 127.5).astype(\"uint8\")\n",
        "          images = np.repeat(images, 3, axis=-1)\n",
        "          vis = build_montages(images, (28, 28), (16, 16))[0]\n",
        "          p = os.path.sep.join(p)\n",
        "          cv2.imwrite(p, vis)\n",
        "\n",
        "    plt.imshow(vis)\n",
        "\n",
        "  def predict(self, noises):\n",
        "    predicted_images = self.Generator.predict(noises , verbose=0)\n",
        "    predicted_images = ((predicted_images * 127.5) + 127.5).astype(\"uint8\")\n",
        "    predicted_images = np.repeat(predicted_images, 3, axis=-1)\n",
        "    vis = build_montages(predicted_images, (28, 28), (16, 16))[0]\n",
        "    return vis\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "mj9oQAGR6dLS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make an instance of our GAN class (our model)\n",
        "my_model = GAN(buffer_size, batch_size)\n"
      ],
      "metadata": {
        "id": "u-s6Tp-7aLqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model...\n",
        "my_model.train(trainImages, 1)"
      ],
      "metadata": {
        "id": "I26eXBLodlVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model by giving random noise\n",
        "res = my_model.predict(tf.random.normal((batch_size,100)))\n",
        "plt.imshow(res)"
      ],
      "metadata": {
        "id": "y353q0lQfdCF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}